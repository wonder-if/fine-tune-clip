datasets_info:
  datasets_root: /mnt/local-data/workspace/datasets/
  datasets_meta:
  - name: office-31
    dir: office-31/
    num_classes: 31
    domains:
    - domain_name: amazon
      domain_id: 0
      domain_abbr: A
      domain_dir: amazon/
    - domain_name: dslr
      domain_id: 1
      domain_abbr: D
      domain_dir: dslr/
    - domain_name: webcam
      domain_id: 2
      domain_abbr: W
      domain_dir: webcam/
  - name: domainnet
    dir: domainnet/
    num_classes: 345
    domains:
    - domain_name: clipart
      domain_id: 0
      domain_abbr: clp
      domain_dir: clipart/
    - domain_name: infograph
      domain_id: 1
      domain_abbr: 'inf'
      domain_dir: infograph/
    - domain_name: painting
      domain_id: 2
      domain_abbr: pnt
      domain_dir: painting/
    - domain_name: quickdraw
      domain_id: 3
      domain_abbr: qdr
      domain_dir: quickdraw/
    - domain_name: real
      domain_id: 4
      domain_abbr: rel
      domain_dir: real/
    - domain_name: sketch
      domain_id: 5
      domain_abbr: skt
      domain_dir: sketch/
  - name: office-home
    dir: office-home/
    num_classes: 65
    domains:
    - domain_name: Art
      domain_id: 0
      domain_abbr: Ar
      domain_dir: Art/
    - domain_name: Clipart
      domain_id: 1
      domain_abbr: Cl
      domain_dir: Clipart/
    - domain_name: Product
      domain_id: 2
      domain_abbr: Pr
      domain_dir: Product/
    - domain_name: Real World
      domain_id: 3
      domain_abbr: Rw
      domain_dir: Real World/
  - name: visda-2017
    dir: visda-2017/
    num_classes: 12
    domains:
    - domain_name: synthetic
      domain_id: 0
      domain_abbr: syn
      domain_dir: train/
    - domain_name: real
      domain_id: 1
      domain_abbr: rel
      domain_dir: validation/
models_info:
  root: /mnt/nas-data/work-data/models/
  models_meta:
  - name: clip-vit-b-16-datacomp.l-s1b-b8k
    path: modelscope/hub/laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K/
    config: ''
  - name: clip-vit-l-14-datacomp.xl-s13b-b90k
    path: modelscope/hub/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/
    config: ''
  - name: clip-vit-base-patch32
    path: modelscope/hub/thomas/clip-vit-base-patch32/
    config: ''
  - name: clip-vit-base-patch16
    path: modelscope/hub/openai-mirror/clip-vit-base-patch16/
    config: ''
dataset:
  dataset_name: office-31
  domain_name: amazon
model:
  model_name: clip-vit-b-16-datacomp.l-s1b-b8k
transform:
  common:
    image_size:
    - 224
    - 224
    image_mean:
    - 0.481
    - 0.458
    - 0.408
    image_std:
    - 0.269
    - 0.262
    - 0.276
  train:
    _target_: torchvision.transforms.Compose
    transforms:
    - _target_: torchvision.transforms.RandomResizedCrop
      size: ${..common.image_size}
      scale:
      - 0.1
      - 1.0
      ratio:
      - 0.75
      - 1.33
      interpolation: BICUBIC
    - _target_: torchvision.transforms.RandomHorizontalFlip
    - _target_: torchvision.transforms.ColorJitter
      brightness: 0.4
      contrast: 0.4
      saturation: 0.4
      hue: 0.2
    - _target_: torchvision.transforms.GaussianBlur
      kernel_size: 5
    - _target_: torchvision.transforms.RandomGrayscale
      p: 0.2
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize
      mean: ${transform.common.image_mean}
      std: ${transform.common.image_std}
  validation:
    _target_: torchvision.transforms.Compose
    transforms:
    - _target_: torchvision.transforms.Resize
      size: ${transform.common.image_size}
      interpolation: BICUBIC
    - _target_: torchvision.transforms.CenterCrop
      size: ${transform.common.image_size}
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize
      mean: ${transform.common.image_mean}
      std: ${transform.common.image_std}
