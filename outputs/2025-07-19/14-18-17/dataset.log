[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - DataManager initialized
[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - Adding dataset: office-31, Domain: amazon
[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset office-31
[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - Adding dataset: office-31, Domain: dslr
[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset office-31
[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - Adding dataset: office-31, Domain: webcam
[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset office-31
[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - Adding dataset: domainnet, Domain: clipart
[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset domainnet
[2025-07-19 14:18:17,385][clip_tuner.data.dataset_manager][INFO] - Adding dataset: domainnet, Domain: infograph
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset domainnet
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Adding dataset: domainnet, Domain: painting
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset domainnet
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Adding dataset: domainnet, Domain: quickdraw
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset domainnet
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Adding dataset: domainnet, Domain: real
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset domainnet
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Adding dataset: domainnet, Domain: sketch
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset domainnet
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Adding dataset: office-home, Domain: Art
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset office-home
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Adding dataset: office-home, Domain: Clipart
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset office-home
[2025-07-19 14:18:17,386][clip_tuner.data.dataset_manager][INFO] - Adding dataset: office-home, Domain: Product
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset office-home
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Adding dataset: office-home, Domain: Real World
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset office-home
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Adding dataset: visda-2017, Domain: synthetic
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset visda-2017
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Adding dataset: visda-2017, Domain: real
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Successfully added dataset visda-2017
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Datasets information loaded successfully
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Getting dataset: office-31, Domain: amazon
[2025-07-19 14:18:17,387][clip_tuner.data.dataset_manager][INFO] - Dataset found: office-31, Domain: amazon
[2025-07-19 14:18:17,389][__main__][INFO] - Dataset({
    features: ['image', 'label'],
    num_rows: 2817
})
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - ModelManager initialized
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Adding model: clip-vit-b-16-datacomp.l-s1b-b8k
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Successfully added modelclip-vit-b-16-datacomp.l-s1b-b8k
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Adding model: clip-vit-l-14-datacomp.xl-s13b-b90k
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Successfully added modelclip-vit-l-14-datacomp.xl-s13b-b90k
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Adding model: clip-vit-base-patch32
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Successfully added modelclip-vit-base-patch32
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Adding model: clip-vit-base-patch16
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Successfully added modelclip-vit-base-patch16
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Models information loaded successfully
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Getting model: clip-vit-base-patch16
[2025-07-19 14:18:17,392][clip_tuner.models.model_manager][INFO] - Model found: clip-vit-base-patch16
[2025-07-19 14:18:17,627][__main__][INFO] - clip_model: CLIPModel(
  (text_model): CLIPTextTransformer(
    (embeddings): CLIPTextEmbeddings(
      (token_embedding): Embedding(49408, 512)
      (position_embedding): Embedding(77, 512)
    )
    (encoder): CLIPEncoder(
      (layers): ModuleList(
        (0-11): 12 x CLIPEncoderLayer(
          (self_attn): CLIPSdpaAttention(
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): CLIPMLP(
            (activation_fn): QuickGELUActivation()
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
          )
          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (vision_model): CLIPVisionTransformer(
    (embeddings): CLIPVisionEmbeddings(
      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (position_embedding): Embedding(197, 768)
    )
    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (encoder): CLIPEncoder(
      (layers): ModuleList(
        (0-11): 12 x CLIPEncoderLayer(
          (self_attn): CLIPSdpaAttention(
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): CLIPMLP(
            (activation_fn): QuickGELUActivation()
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (visual_projection): Linear(in_features=768, out_features=512, bias=False)
  (text_projection): Linear(in_features=512, out_features=512, bias=False)
)
[2025-07-19 14:18:17,627][__main__][INFO] - clip_tokenizer: CLIPTokenizer(name_or_path='/mnt/local-data/workspace/models/modelscope/hub/openai-mirror/clip-vit-base-patch16/', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	49406: AddedToken("<|startoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	49407: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
[2025-07-19 14:18:17,703][__main__][INFO] - clip_processor: CLIPProcessor:
- image_processor: CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}

- tokenizer: CLIPTokenizerFast(name_or_path='/mnt/local-data/workspace/models/modelscope/hub/openai-mirror/clip-vit-base-patch16/', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	49406: AddedToken("<|startoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	49407: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)

{
  "processor_class": "CLIPProcessor"
}

