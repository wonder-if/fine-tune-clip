2025-03-26 00:05:05 [INFO] [logger.py] [info] [line 49] ModelManager initialized
2025-03-26 00:05:05 [INFO] [logger.py] [info] [line 49] Loading models from JSON file: /mnt/nas-data/work-data/programs/2025-03-14-fine-tune-clip/clip_tuner/models/clip_tuner/configs/models_config.json
2025-03-26 00:05:05 [ERROR] [logger.py] [error] [line 55] Error loading models from JSON file: [Errno 2] No such file or directory: '/mnt/nas-data/work-data/programs/2025-03-14-fine-tune-clip/clip_tuner/models/clip_tuner/configs/models_config.json'
2025-03-26 00:05:05 [INFO] [logger.py] [info] [line 49] Listing all models
2025-03-26 00:06:48 [INFO] [logger.py] [info] [line 49] ModelManager initialized
2025-03-26 00:06:48 [INFO] [logger.py] [info] [line 49] Loading models from JSON file: /mnt/nas-data/work-data/programs/2025-03-14-fine-tune-clip/clip_tuner/clip_tuner/configs/models_info.json
2025-03-26 00:06:48 [ERROR] [logger.py] [error] [line 55] Error loading models from JSON file: [Errno 2] No such file or directory: '/mnt/nas-data/work-data/programs/2025-03-14-fine-tune-clip/clip_tuner/clip_tuner/configs/models_info.json'
2025-03-26 00:06:48 [INFO] [logger.py] [info] [line 49] Listing all models
2025-03-26 00:07:42 [INFO] [logger.py] [info] [line 49] ModelManager initialized
2025-03-26 00:07:42 [INFO] [logger.py] [info] [line 49] Loading models from JSON file: /mnt/nas-data/work-data/programs/2025-03-14-fine-tune-clip/clip_tuner/configs/models_info.json
2025-03-26 00:07:42 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:07:43 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:07:43 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-base-patch32
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-base-patch32
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-base-patch32
2025-03-26 00:07:43 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-base-patch16
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-base-patch16
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-base-patch16
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Models information loaded successfully
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Listing all models
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Model: clip-vit-b-16-datacomp.l-s1b-b8k, Path: /mnt/nas-data/work-data/models/modelscope/hub/laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K/, Config: 
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Model: clip-vit-l-14-datacomp.xl-s13b-b90k, Path: /mnt/nas-data/work-data/models/modelscope/hub/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/, Config: 
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Model: clip-vit-base-patch32, Path: /mnt/nas-data/work-data/models/modelscope/hub/thomas/clip-vit-base-patch32/, Config: 
2025-03-26 00:07:43 [INFO] [logger.py] [info] [line 49] Model: clip-vit-base-patch16, Path: /mnt/nas-data/work-data/models/modelscope/hub/openai-mirror/clip-vit-base-patch16/, Config: 
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] ModelManager initialized
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Loading models from JSON file: /mnt/nas-data/work-data/programs/2025-03-14-fine-tune-clip/clip_tuner/configs/models_info.json
2025-03-26 00:14:57 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:14:57 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:14:57 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-base-patch32
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-base-patch32
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-base-patch32
2025-03-26 00:14:57 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-base-patch16
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-base-patch16
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-base-patch16
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Models information loaded successfully
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Listing all models
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Model: clip-vit-b-16-datacomp.l-s1b-b8k, Path: /mnt/nas-data/work-data/models/modelscope/hub/laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K/, Config: 
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Model: clip-vit-l-14-datacomp.xl-s13b-b90k, Path: /mnt/nas-data/work-data/models/modelscope/hub/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/, Config: 
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Model: clip-vit-base-patch32, Path: /mnt/nas-data/work-data/models/modelscope/hub/thomas/clip-vit-base-patch32/, Config: 
2025-03-26 00:14:57 [INFO] [logger.py] [info] [line 49] Model: clip-vit-base-patch16, Path: /mnt/nas-data/work-data/models/modelscope/hub/openai-mirror/clip-vit-base-patch16/, Config: 
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] ModelManager initialized
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Loading models from JSON file: /mnt/nas-data/work-data/programs/2025-03-14-fine-tune-clip/clip_tuner/configs/models_info.json
2025-03-26 00:15:33 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:15:33 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:15:33 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-base-patch32
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-base-patch32
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-base-patch32
2025-03-26 00:15:33 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-base-patch16
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-base-patch16
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-base-patch16
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Models information loaded successfully
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Listing all models
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Model: clip-vit-b-16-datacomp.l-s1b-b8k, Path: /mnt/nas-data/work-data/models/modelscope/hub/laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K/, Config: 
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Model: clip-vit-l-14-datacomp.xl-s13b-b90k, Path: /mnt/nas-data/work-data/models/modelscope/hub/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/, Config: 
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Model: clip-vit-base-patch32, Path: /mnt/nas-data/work-data/models/modelscope/hub/thomas/clip-vit-base-patch32/, Config: 
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Model: clip-vit-base-patch16, Path: /mnt/nas-data/work-data/models/modelscope/hub/openai-mirror/clip-vit-base-patch16/, Config: 
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Getting model: clip-vit-base-patch16
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] Model found: clip-vit-base-patch16
2025-03-26 00:15:33 [INFO] [logger.py] [info] [line 49] CLIPModel(
  (text_model): CLIPTextTransformer(
    (embeddings): CLIPTextEmbeddings(
      (token_embedding): Embedding(49408, 512)
      (position_embedding): Embedding(77, 512)
    )
    (encoder): CLIPEncoder(
      (layers): ModuleList(
        (0-11): 12 x CLIPEncoderLayer(
          (self_attn): CLIPSdpaAttention(
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): CLIPMLP(
            (activation_fn): QuickGELUActivation()
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
          )
          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (vision_model): CLIPVisionTransformer(
    (embeddings): CLIPVisionEmbeddings(
      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (position_embedding): Embedding(197, 768)
    )
    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (encoder): CLIPEncoder(
      (layers): ModuleList(
        (0-11): 12 x CLIPEncoderLayer(
          (self_attn): CLIPSAttention(
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): CLIPMLP(
            (activation_fn): QuickGELUActivation()
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (visual_projection): Linear(in_features=768, out_features=512, bias=False)
  (text_projection): Linear(in_features=512, out_features=512, bias=False)
)
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] ModelManager initialized
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Loading models from JSON file: /mnt/nas-data/work-data/programs/2025-03-14-fine-tune-clip/clip_tuner/configs/models_info.json
2025-03-26 00:41:24 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-b-16-datacomp.l-s1b-b8k
2025-03-26 00:41:24 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-l-14-datacomp.xl-s13b-b90k
2025-03-26 00:41:24 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-base-patch32
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-base-patch32
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-base-patch32
2025-03-26 00:41:24 [DEBUG] [logger.py] [debug] [line 46] Processing model: clip-vit-base-patch16
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Adding model: clip-vit-base-patch16
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Successfully added modelclip-vit-base-patch16
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Models information loaded successfully
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Listing all models
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Model: clip-vit-b-16-datacomp.l-s1b-b8k, Path: /mnt/nas-data/work-data/models/modelscope/hub/laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K/, Config: 
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Model: clip-vit-l-14-datacomp.xl-s13b-b90k, Path: /mnt/nas-data/work-data/models/modelscope/hub/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/, Config: 
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Model: clip-vit-base-patch32, Path: /mnt/nas-data/work-data/models/modelscope/hub/thomas/clip-vit-base-patch32/, Config: 
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Model: clip-vit-base-patch16, Path: /mnt/nas-data/work-data/models/modelscope/hub/openai-mirror/clip-vit-base-patch16/, Config: 
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Getting model: clip-vit-base-patch16
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] Model found: clip-vit-base-patch16
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] CLIPTokenizer(name_or_path='/mnt/nas-data/work-data/models/modelscope/hub/openai-mirror/clip-vit-base-patch16/', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	49406: AddedToken("<|startoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	49407: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
2025-03-26 00:41:24 [INFO] [logger.py] [info] [line 49] CLIPModel(
  (text_model): CLIPTextTransformer(
    (embeddings): CLIPTextEmbeddings(
      (token_embedding): Embedding(49408, 512)
      (position_embedding): Embedding(77, 512)
    )
    (encoder): CLIPEncoder(
      (layers): ModuleList(
        (0-11): 12 x CLIPEncoderLayer(
          (self_attn): CLIPSdpaAttention(
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): CLIPMLP(
            (activation_fn): QuickGELUActivation()
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
          )
          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (vision_model): CLIPVisionTransformer(
    (embeddings): CLIPVisionEmbeddings(
      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (position_embedding): Embedding(197, 768)
    )
    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (encoder): CLIPEncoder(
      (layers): ModuleList(
        (0-11): 12 x CLIPEncoderLayer(
          (self_attn): CLIPSdpaAttention(
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): CLIPMLP(
            (activation_fn): QuickGELUActivation()
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (visual_projection): Linear(in_features=768, out_features=512, bias=False)
  (text_projection): Linear(in_features=512, out_features=512, bias=False)
)
